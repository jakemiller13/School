{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a href=\"https://www.bigdatauniversity.com\"><img src=\"https://ibm.box.com/shared/static/qo20b88v1hbjztubt06609ovs85q8fau.png\" width=\"400px\" align=\"center\"></a>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<h1 align=\"center\"><font size=\"5\">Project: Character Modeling</font></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<font size=\"3\"><strong>In this notebook you will use TensorFlow to create a Recurrent Neural Network, to predict the next character in a string. You need to train your network using a CPU and using a GPU and benchmark the result to see which which device You have to write your code in empty cells in this notebook to make it complete, and then submit the notebook for peer-review.</strong></font>\n",
    "\n",
    "<h2>Table of Contents</h2>\n",
    "<ul>\n",
    "    <li><a href=\"#intro\">Introduction</a></li>\n",
    "    <li><a href=\"#lstm\">Long Short-Term Memory Model (LSTM) Architectures</a></li>\n",
    "    <li><a href=\"#cpu_vs_gpu\">Train your model using CPU and GPU</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#question_1\">Question 1: Complete the code to run it on CPU</a></li>\n",
    "            <li><a href=\"#question_2\">Question 2: Complete the code to run it on GPU</a></li>\n",
    "            <li><a href=\"#question_3\">Question 3: Compare the results</a></li>\n",
    "        </ol>    \n",
    "    </li>\n",
    "</ul>\n",
    "<p></p>\n",
    "</div>\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"intro\"></a>\n",
    "<h2>Introduction</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>This code is supposed to implement a Recurrent Neural Network with LSTM units for training/sampling from character-level language models. In other words, the model takes a text file as input and trains the RNN network that learns to predict the next character in a sequence.</p>  \n",
    "The RNN can then be used to generate text character by character that will look like the original training data. \n",
    "\n",
    "<p>This code is based on this <a href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\">blog</a>, and the code is an step-by-step implementation of the <a href=\"https://github.com/crazydonkey200/tensorflow-char-rnn\">character-level implimentation</a>.</p>\n",
    "\n",
    "<p>I recommend you to complete the \"<a href=\"https://www.edx.org/course/deep-learning-with-tensorflow\">Deep Learning with TensorFlow</a>\" course for this project.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "First, lets import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import codecs\n",
    "import os\n",
    "import collections\n",
    "from six.moves import cPickle\n",
    "import numpy as np\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.log_device_placement = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Libraries\n",
    "<ul>\n",
    "    <li><b>os</b>: is an module that allows us to interact with the operating systerm, in particular we will use it to set the path in which we will be storing our input file, tensor file and vocab file</li>\n",
    "    <li><b>time</b>: is a library that allows us to access the clock time of our machine, we will use it to measure the performance of training our model with a CPU, versus training our model with a GPU</li>\n",
    "    <li><b>cPickle</b>: is a library for serializing and deserializing python objects, we will use the <b>dump()</b> method in cPickle to serialize our objects when saving them, and <b>load()</b> method in cPickle to deserialize our objects when loading.</li>\n",
    "    <li><b>codec</b>: is a library that deals with character encoding, we will use the <b>open()</b> method as it is recommended when opening encoded text files.</li>\n",
    "    <li><b>collections</b>: is a library that implements high performance container types, we will use the <b>Counter</b> object to get a collection of frequencies for our characters</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Downloading the input data</h3>\n",
    "Lets download the input file, and take a look at some parts of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-31 20:02:06 URL:https://public.boxcloud.com/d/1/b1!Q97Bzr3RHSdr-HQAdY65kFHmufM27UffQ1u-TyCavmS_S9z7OJ_VRE2DsizL7uMVhBSxsCvqWnjSbm2vAXCiN9eFFJviOg1iLKSlvaUqSFdkKbdA-OwZt5z1JW2y8ptNEHa5ZdDkIb9UCjkhPGpNahbkOZBTK59geq0ba4z6Byj_3qdESX62Zu6aXaHHOku26eNW_LS2nLJH-eXWidCIU-7fOwecPLEijRirati-VuRN0ZEk5UysSF5F1CN6nlZXMBKGnRBEzOV0XQNyTx9oO9PyzkMGA3oEBcEhJiya_xWK6zWEYm4kCFQffdSDlL3l2NVdeyw6i5Xsskac9DDoJwsCYxGhsWbBKfqpw1JDRrxhe0p8v3auGDlseqJWwXgBpDtzmcKom9pGVJU0exOaCtGXec9r88Ceno8enhdYOYC9l94I1I8zb5chETnR1KN7PsrXAqZPprpFZGX9BKSwYKrmg-fWUIDRS68hBdUBh86AVmm9ZoCAYm6mgwiZn5NXVPojof2Y_T4SOMKcqA1W94qgsKSFlL08iVh61XcXF_iPfUMumGnXpD6o1pOEjTNUP1gOOV4l4YmdEUj37MklS-t0RrBgvF4QHgPnVdQbbjnLe-z67AAp_oJH8eUEM7QB5ced7H2tK9JcAqxvPm772nMclW1bdSkF7HT9_JkXxWZ3lpGmBwhGq8enhx7eTmAMGdPUwxk75QQKtCAO7bxYtfPfFwZuMIhvQAoWZckRT5nazRwsiGOkTZGQRFpgUbowcFUaqEKwytYGIeoTo5PWw9qz7ejPi7QMTFqrVOfla_MNJxHSDGV6aviN10ukuYvwaO5dGadL741oR6fEqqLQQMvlJ7BS1doYE8fPs36H7BUTsLW7QJ6GNjfqMaTrbRCWYOzMVMsv8mCZUQ0qVLMNJehhw24x7FI5MJBTJBz4BR2VbCitMwF7qMKEUUSQPPjKTI40ODEIPPFooDX8NQFGMb9WsbdGfPmmG3Qs3qjmh9K1EU9MENcPgIkYtfzWQeRbn56gukGUaGi-uUfEu_z4hgRGwe6UjtXmF1qFvENd1d9NPlhUZxsk24MB_yKpsZVW7MK4mVJAj_k5cYyiw9A87PsGMuvPW9jIqGTlJIDI_nGPhYJA2_jJzOfo2trCw4gqrYCuXim_ZDh1a0O0oMuipBITUE74Hilu0QGGLY_dxCd13N_9eI3qez3bsLFggjBsHL5oJRyv_4LOxerx44D3Gb8tvvxvkiZo-00IYF9U00XXKq3tLXlxj0FnSaJTe3jXeid2pQvaGQjzRHbj0YMyh0XkpuKp-PJrZdXt02ss7ftOWI7hvIAMpVvY4t61qYSR0pauIsDm94nIoz8wBIpega6TkuMZBk5_8AKnVVZB-qIXqw../download [1115393/1115393] -> \"input.txt\" [1]\n",
      "-------------Sample text---------------\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "---------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget -nv -O input.txt https://ibm.box.com/shared/static/a3f9e9mbpup09toq35ut7ke3l3lf03hg.txt \n",
    "with open('input.txt', 'r') as f:\n",
    "    read_data = f.read()\n",
    "    print(\"-------------Sample text---------------\")\n",
    "    print (read_data[0:500])\n",
    "    print(\"---------------------------------------\")\n",
    "f.closed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Data loader</h3>\n",
    "You need to read the input file and convert each character to numerical values. The following cell is a class that helps to read data from input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class TextLoader():\n",
    "    def __init__(self, data_dir, batch_size, seq_length, encoding='utf-8'):\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "        self.encoding = encoding\n",
    "\n",
    "        input_file = os.path.join(data_dir, \"input.txt\")\n",
    "        vocab_file = os.path.join(data_dir, \"vocab.pkl\")\n",
    "        tensor_file = os.path.join(data_dir, \"data.npy\")\n",
    "\n",
    "        if not (os.path.exists(vocab_file) and os.path.exists(tensor_file)):\n",
    "            print(\"reading text file\")\n",
    "            self.preprocess(input_file, vocab_file, tensor_file)\n",
    "        else:\n",
    "            print(\"loading preprocessed files\")\n",
    "            self.load_preprocessed(vocab_file, tensor_file)\n",
    "        self.create_batches()\n",
    "        self.reset_batch_pointer()\n",
    "\n",
    "    def preprocess(self, input_file, vocab_file, tensor_file):\n",
    "        with codecs.open(input_file, \"r\", encoding=self.encoding) as f:\n",
    "            data = f.read()\n",
    "        counter = collections.Counter(data)\n",
    "        count_pairs = sorted(counter.items(), key=lambda x: -x[1])\n",
    "        self.chars, _ = zip(*count_pairs)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        with open(vocab_file, 'wb') as f:\n",
    "            cPickle.dump(self.chars, f)\n",
    "        self.tensor = np.array(list(map(self.vocab.get, data)))\n",
    "        np.save(tensor_file, self.tensor)\n",
    "\n",
    "    def load_preprocessed(self, vocab_file, tensor_file):\n",
    "        with open(vocab_file, 'rb') as f:\n",
    "            self.chars = cPickle.load(f)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.vocab = dict(zip(self.chars, range(len(self.chars))))\n",
    "        self.tensor = np.load(tensor_file)\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "    def create_batches(self):\n",
    "        self.num_batches = int(self.tensor.size / (self.batch_size * self.seq_length))\n",
    "\n",
    "        # When the data (tensor) is too small, let's give them a better error message\n",
    "        if self.num_batches==0:\n",
    "            assert False, \"Not enough data. Make seq_length and batch_size small.\"\n",
    "\n",
    "        self.tensor = self.tensor[:self.num_batches * self.batch_size * self.seq_length]\n",
    "        xdata = self.tensor\n",
    "        ydata = np.copy(self.tensor)\n",
    "        ydata[:-1] = xdata[1:]\n",
    "        ydata[-1] = xdata[0]\n",
    "        self.x_batches = np.split(xdata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "        self.y_batches = np.split(ydata.reshape(self.batch_size, -1), self.num_batches, 1)\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        x, y = self.x_batches[self.pointer], self.y_batches[self.pointer]\n",
    "        self.pointer += 1\n",
    "        return x, y\n",
    "\n",
    "    def reset_batch_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>Parameters</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>We have to convert the characters in the string to numbers. Also we need to represent each sequence of characters as a vector in each batch.</p>\n",
    "So, let's set some parameters that we need those now for reading the dataset, and later to build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "seq_length = 50 # RNN sequence length\n",
    "batch_size = 128  # minibatch size, i.e. size of data in each epoch\n",
    "num_epochs = 50 # you should change it to 50 if you want to see a relatively good results <- was 20, changed to 50\n",
    "learning_rate = 0.002\n",
    "decay_rate = 0.97\n",
    "rnn_size = 128 # size of RNN hidden state (output dimension)\n",
    "num_layers = 2 #number of layers in the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>Now, we can read the data at batches using the <b>TextLoader</b> class. It will convert the characters to numbers, and represent each sequence as a vector in batches:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading preprocessed files\n",
      "vocabulary size: 65\n",
      "Characters: (' ', 'e', 't', 'o', 'a', 'h', 's', 'r', 'n', 'i', '\\n', 'l', 'd', 'u', 'm', 'y', ',', 'w', 'f', 'c', 'g', 'I', 'b', 'p', ':', '.', 'A', 'v', 'k', 'T', \"'\", 'E', 'O', 'N', 'R', 'S', 'L', 'C', ';', 'W', 'U', 'H', 'M', 'B', '?', 'G', '!', 'D', '-', 'F', 'Y', 'P', 'K', 'V', 'j', 'q', 'x', 'z', 'J', 'Q', 'Z', 'X', '3', '&', '$')\n",
      "vocab number of 'F': 49\n",
      "Character sequences (first batch): [[49  9  7 ...  1  4  7]\n",
      " [39  5  3 ...  0 20  9]\n",
      " [ 0  5  9 ... 19  4 13]\n",
      " ...\n",
      " [ 3 18 18 ...  1  0 23]\n",
      " [ 7  1 23 ... 18  3  7]\n",
      " [47 26 24 ...  0  8  3]]\n"
     ]
    }
   ],
   "source": [
    "data_loader = TextLoader('', batch_size, seq_length)\n",
    "vocab_size = data_loader.vocab_size\n",
    "print (\"vocabulary size:\" , data_loader.vocab_size)\n",
    "print (\"Characters:\" , data_loader.chars)\n",
    "print (\"vocab number of 'F':\", data_loader.vocab['F'])\n",
    "print (\"Character sequences (first batch):\", data_loader.x_batches[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>Notice:</b> In the following cells, we just go through the process of defining each element of the LSTM, and explore the inputs, outputs of each layer. Then, we put all together and run the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>1- Input and Output</h3>\n",
    "In the next cell we just take a look at a sample batch to underestand the data better. Each batch includes the input, <b>x</b>, and the character that we want to predict, <b>y</b>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [39,  5,  3, ...,  0, 20,  9],\n",
       "       [ 0,  5,  9, ..., 19,  4, 13],\n",
       "       ...,\n",
       "       [ 3, 18, 18, ...,  1,  0, 23],\n",
       "       [ 7,  1, 23, ..., 18,  3,  7],\n",
       "       [47, 26, 24, ...,  0,  8,  3]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = data_loader.next_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape  #batch_size =128, seq_length=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here, __y__ is the next character for each character in __x__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  6, ...,  4,  7,  0],\n",
       "       [ 5,  3,  0, ..., 20,  9, 27],\n",
       "       [ 5,  9, 14, ...,  4, 13, 20],\n",
       "       ...,\n",
       "       [18, 18,  9, ...,  0, 23, 11],\n",
       "       [ 1, 23,  3, ...,  3,  7,  0],\n",
       "       [26, 24, 10, ...,  8,  3,  2]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"lstm\"></a>\n",
    "<h3>2- What is Long Short-Term Memory Model (LSTM)?</h3>\n",
    "\n",
    "<p>Recurrent Neural Networks are Deep Learning models with simple structures and a feedback mechanism built-in, or in different words, the output of a layer is added to the next input and fed back to the same layer.</p>\n",
    "\n",
    "<p>The Recurrent Neural Network is a specialized type of Neural Network that solves the issue of <b>maintaining context for Sequential data</b> -- such as Weather data, Stocks, Genes, etc. At each iterative step, the processing unit takes in an input and the current state of the network, and produces an output and a new state that is <b>re-fed into the network</b>.</p>\n",
    "\n",
    "<p>However, <b>this model has some problems</b>. It's very computationally expensive to maintain the state for a large amount of units, even more so over a long amount of time. Additionally, Recurrent Networks are very sensitive to changes in their parameters. To solve these problems, we use a specific type of RNN, is called Long Short-Term Memory (LSTM).</p>\n",
    "\n",
    "\n",
    "Each LSTM cell has 5 parts:\n",
    "<ol>\n",
    "    <li>Input</li>\n",
    "    <li>prv_state</li>\n",
    "    <li>prv_output</li>\n",
    "    <li>new_state</li>\n",
    "    <li>new_output</li>\n",
    "</ol>\n",
    "\n",
    "<ul>\n",
    "    <li>Each LSTM cell has an input layer, which its size is 128 units in our case. The input vector's dimension also is 128, which is the dimensionality of embedding vector, so called, dimension size of Word2Vec embedding, for each character.</li>\n",
    "    <li>Each LSTM cell has a hidden layer, where there are some hidden units. The argument n_hidden=128 of BasicLSTMCell is the number of hidden units of the LSTM (inside A). It keeps the size of the output and state vector. It is also known as, rnn_size, num_units, num_hidden_units, and LSTM size, in literature.</li>\n",
    "    <li>An LSTM keeps two pieces of information as it propagates through time:</li> \n",
    "    <ul>\n",
    "         <li><b>hidden state</b> vector: Each LSTM cell accept a vector, called <b>hidden state</b> vector, of size n_hidden=128, and its value is returned to the LSTM cell in the next step. The <b>hidden state</b> vector; which is the memory of the LSTM, accumulates using its (forget, input, and output) gates through time. \"num_units\" is equivalant to \"size of RNN hidden state\". Number of hidden units is the dimensianality of the output (= dimesianality of the state) of the LSTM cell.</li>\n",
    "        <li><b>previous time-step output</b>: For each LSTM cell that we initialize, we need to supply a value (128 in this case) for the hidden dimension, or as some people like to call it, the number of units in the LSTM cell.</li> \n",
    "    </ul>\n",
    "</ul>\n",
    "<br>\n",
    "\n",
    "<h4>Stacked LSTM</h4>\n",
    "<p>What about if we want to have a RNN with stacked LSTM? For example, a 2-layer LSTM. In this case, the output of the first layer will become the input of the second.</p>\n",
    "\n",
    "num_layers = 2 \n",
    "<ul>\n",
    "    <li>number of layers in the RNN, is defined by <code>num_layers</code> parameter.</li>\n",
    "    <li>An input of MultiRNNCell is <b>cells</b> which is list of RNNCells that will be composed in this order.</li>\n",
    "</ul>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>3- Defining stacked RNN Cell</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>BasicRNNCell</b> is the most basic RNN cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to define a LSTM cell\n",
    "cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "# a two layer cell\n",
    "stacked_cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "# hidden state size\n",
    "stacked_cell.output_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<b>state</b> variable keeps output and new_state of the LSTM, so it is double in size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 128)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets define the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder:0' shape=(128, 50) dtype=int32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = tf.placeholder(tf.int32, [batch_size, seq_length])# a 128x50\n",
    "input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "and target data, what we want to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_1:0' shape=(128, 50) dtype=int32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = tf.placeholder(tf.int32, [batch_size, seq_length]) # a 128x50\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The memory state of the network is initialized with a vector of zeros and gets updated after reading each character.\n",
    "\n",
    "<b>BasicRNNCell.zero_state(batch_size, dtype)</b> Return zero-filled state tensor(s). In this function, batch_size\n",
    "representing the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState/zeros:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'MultiRNNCellZeroState/BasicRNNCellZeroState_1/zeros:0' shape=(128, 128) dtype=float32>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_state = stacked_cell.zero_state(batch_size, tf.float32) \n",
    "initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets check the value of the input_data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[49,  9,  7, ...,  1,  4,  7],\n",
       "       [39,  5,  3, ...,  0, 20,  9],\n",
       "       [ 0,  5,  9, ..., 19,  4, 13],\n",
       "       ...,\n",
       "       [ 3, 18, 18, ...,  1,  0, 23],\n",
       "       [ 7,  1, 23, ..., 18,  3,  7],\n",
       "       [47, 26, 24, ...,  0,  8,  3]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session = tf.Session(config=config)\n",
    "feed_dict={input_data:x, targets:y}\n",
    "session.run(input_data, feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>4- Embedding</h3>\n",
    "<p>In this section, we build a 128-dim vector for each character. As we have 60 batches, and 50 character in each sequence, it will generate a [60,50,128] matrix.</p>\n",
    "\n",
    "<p><b>Notice:</b> The function <code>tf.get_variable()</code> is used to share a variable and to initialize it in one place. <code>tf.get_variable()</code> is used to get or create a variable instead of a direct call to <code>tf.Variable</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('rnnlm', reuse=False):\n",
    "    softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "    softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65)\n",
    "    #with tf.device(\"/cpu:0\"):\n",
    "        \n",
    "    # embedding variable is initialized randomely\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "\n",
    "    # embedding_lookup goes to each row of input_data, and for each character in the row, finds the correspond vector in embedding\n",
    "    # it creates a 60*50*[1*128] matrix\n",
    "    # so, the first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character\n",
    "    em = tf.nn.embedding_lookup(embedding, input_data) # em is 60x50x[1*128]\n",
    "    # split: Splits a tensor into sub tensors.\n",
    "    # syntax:  tf.split(split_dim, num_split, value, name='split')\n",
    "    # it will split the 60x50x[1x128] matrix into 50 matrix of 60x[1*128]\n",
    "    inputs = tf.split(em, seq_length, 1)\n",
    "    # It will convert the list to 50 matrix of [60x128]\n",
    "    inputs = [tf.squeeze(input_, [1]) for input_ in inputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Lets take a look at the <b>embedding</b>, <b>em</b>, and <b>inputs</b> variables:\n",
    "\n",
    "Embedding variable is initialized with random values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.05005839, -0.164846  , -0.16913219, ..., -0.08180564,\n",
       "        -0.16002409, -0.06438529],\n",
       "       [ 0.11869372,  0.08183123,  0.03263192, ...,  0.03091742,\n",
       "        -0.05609655,  0.01552239],\n",
       "       [ 0.12159596, -0.13446099,  0.02099597, ..., -0.13576134,\n",
       "         0.01912594,  0.00428034],\n",
       "       ...,\n",
       "       [ 0.17111565,  0.01509239,  0.12681688, ..., -0.17224978,\n",
       "         0.1442327 ,  0.04335518],\n",
       "       [ 0.1403156 ,  0.15923165, -0.11274803, ..., -0.12823835,\n",
       "        -0.01049021, -0.17008165],\n",
       "       [-0.09553652, -0.05007601,  0.14189292, ...,  0.17273657,\n",
       "        -0.10281775,  0.15545963]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "print(embedding.shape)\n",
    "session.run(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "The first elemnt of em, is a matrix of 50x128, which each row of it is vector representing that character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 50, 128)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.08286973,  0.07669862,  0.15940739, ...,  0.10282247,\n",
       "         0.0113368 ,  0.0987732 ],\n",
       "       [-0.12599492, -0.10932873, -0.06541151, ...,  0.17094286,\n",
       "        -0.05113523, -0.12873274],\n",
       "       [ 0.06598549,  0.05864687, -0.14084536, ...,  0.11818643,\n",
       "        -0.02560635,  0.01063658],\n",
       "       ...,\n",
       "       [ 0.11869372,  0.08183123,  0.03263192, ...,  0.03091742,\n",
       "        -0.05609655,  0.01552239],\n",
       "       [ 0.1372476 ,  0.14193551,  0.08144091, ...,  0.05013087,\n",
       "        -0.11449724, -0.1068764 ],\n",
       "       [ 0.06598549,  0.05864687, -0.14084536, ...,  0.11818643,\n",
       "        -0.02560635,  0.01063658]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em = tf.nn.embedding_lookup(embedding, input_data)\n",
    "emp = session.run(em,feed_dict={input_data:x})\n",
    "print (emp.shape)\n",
    "emp[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>Let's consider each sequence as a sentence of length 50 characters, then, the first item in <b>inputs</b> is a [60x128] vector which represents the first characters of 60 sentences.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'Squeeze:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_1:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_2:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_3:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'Squeeze_4:0' shape=(128, 128) dtype=float32>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tf.split(em, seq_length, 1)\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in inputs]\n",
    "inputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h3>5- Feeding a batch of 50 sequence to a RNN:</h3>\n",
    "\n",
    "The feeding process for inputs is as following:\n",
    "<ul>\n",
    "    <li>Step 1: first character of each of the 50 sentences (in a batch) is entered in parallel.</li>  \n",
    "    <li>Step 2: second character of each of the 50 sentences is input in parallel.</li> \n",
    "    <li>Step n: nth character of each of the 50 sentences is input in parallel.</li>  \n",
    "</ul>\n",
    "<p>The parallelism is only for efficiency. Each character in a batch is handled in parallel, but the network sees one character of a sequence at a time and does the computations accordingly. All the computations involving the characters of all sequences in a batch at a given time step are done in parallel.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08286973,  0.07669862,  0.15940739, ...,  0.10282247,\n",
       "         0.0113368 ,  0.0987732 ],\n",
       "       [-0.02708288,  0.16456051, -0.1447902 , ..., -0.01288046,\n",
       "         0.06417258,  0.00543427],\n",
       "       [ 0.05005839, -0.164846  , -0.16913219, ..., -0.08180564,\n",
       "        -0.16002409, -0.06438529],\n",
       "       ...,\n",
       "       [ 0.09600924, -0.09898961,  0.05661878, ...,  0.10019554,\n",
       "        -0.15097249, -0.17539325],\n",
       "       [ 0.06598549,  0.05864687, -0.14084536, ...,  0.11818643,\n",
       "        -0.02560635,  0.01063658],\n",
       "       [-0.00129883, -0.03466825, -0.09904535, ...,  0.11556698,\n",
       "        -0.06265313,  0.07262371]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(inputs[0],feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Feeding the RNN with one batch, we can check the new output and new state of network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_98:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_99:0' shape=(128, 128) dtype=float32>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputs is 50x[60*128]\n",
    "outputs, new_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_state, stacked_cell, loop_function=None, scope='rnnlm')\n",
    "new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_1:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_3:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_5:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_7:0' shape=(128, 128) dtype=float32>,\n",
       " <tf.Tensor 'rnnlm_1/rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/Tanh_9:0' shape=(128, 128) dtype=float32>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Let's check the output of network after feeding it with first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01889511, -0.01135159,  0.11907647, ..., -0.04444603,\n",
       "         0.04391498,  0.0038373 ],\n",
       "       [ 0.05204726, -0.0875124 ,  0.00135571, ...,  0.06589209,\n",
       "         0.03815315,  0.04973407],\n",
       "       [ 0.02815058,  0.00435396, -0.11997713, ...,  0.09993885,\n",
       "         0.07109024, -0.04996781],\n",
       "       ...,\n",
       "       [-0.05852821,  0.11413842, -0.06913213, ..., -0.00244813,\n",
       "         0.02788062, -0.1138159 ],\n",
       "       [-0.03159584,  0.05121949,  0.07297759, ..., -0.08107258,\n",
       "         0.08940622, -0.01064056],\n",
       "       [-0.04049902, -0.06310745,  0.14767714, ...,  0.01706438,\n",
       "        -0.11151629,  0.12343413]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_output = outputs[0]\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(first_output,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<p>As it was explained, <b>outputs</b> variable is a 50x[60x128] tensor. We need to reshape it back to [60x50x128] to be able to calculate the probablity of the next character using the softmax. The <b>softmax_w</b> shape is [rnn_size, vocab_size], which is [128x65] in our case. Therefore, we have a fully connected layer on top of LSTM cells, which help us to decode the next charachter. We can use the <b>softmax(output * softmax_w + softmax_b)</b> for this purpose. The shape of the matrixis would be:</p>\n",
    "\n",
    "softmax([60x50x128]x[128x65]+[1x65]) = [60x50x65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We can do it step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape:0' shape=(6400, 128) dtype=float32>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.reshape(tf.concat( outputs,1), [-1, rnn_size])\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(6400, 65) dtype=float32>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(6400, 65) dtype=float32>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = tf.nn.softmax(logits)\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Here is the probablity of the next chracter in all batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0166996 , 0.01772449, 0.01660083, ..., 0.01410264, 0.01730376,\n",
       "        0.01349106],\n",
       "       [0.01616266, 0.01351571, 0.01517673, ..., 0.01243294, 0.0188073 ,\n",
       "        0.01269872],\n",
       "       [0.01532092, 0.01600143, 0.01836681, ..., 0.01114   , 0.0149603 ,\n",
       "        0.01475507],\n",
       "       ...,\n",
       "       [0.01424962, 0.01346246, 0.0168163 , ..., 0.01279302, 0.01738641,\n",
       "        0.01138778],\n",
       "       [0.01134322, 0.01573297, 0.01598359, ..., 0.01128589, 0.01935188,\n",
       "        0.01337521],\n",
       "       [0.01627343, 0.01807679, 0.01650165, ..., 0.01309246, 0.02001806,\n",
       "        0.01173284]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.run(tf.global_variables_initializer())\n",
    "session.run(probs,feed_dict={input_data:x})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now, we are in the position to calculate the cost of training with <b>loss function</b>, and keep feeding the network to learn it. But, the question is: what does the LSTM networks learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'rnnlm/softmax_w:0' shape=(128, 65) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/softmax_b:0' shape=(65,) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/embedding:0' shape=(65, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/kernel:0' shape=(256, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'rnnlm/multi_rnn_cell/cell_0/basic_rnn_cell/bias:0' shape=(128,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_clip =5.\n",
    "tvars = tf.trainable_variables()\n",
    "tvars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Okay, by now, you should understand enough about each component of a LSTM network to be able to train it, and predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h2>All together</h2>\n",
    "Now, let's put all of parts together in a class, and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "class LSTMModel():\n",
    "    def __init__(self,sample=False, device='/cpu:0'):\n",
    "        rnn_size = 128 # size of RNN hidden state vector\n",
    "        batch_size = 128 # minibatch size, i.e. size of dataset in each epoch\n",
    "        seq_length = 50 # RNN sequence length\n",
    "        num_layers = 2 # number of layers in the RNN\n",
    "        vocab_size = 65\n",
    "        grad_clip = 5.\n",
    "        if sample:\n",
    "            batch_size = 1\n",
    "            seq_length = 1\n",
    "        with tf.device(device):\n",
    "            # The core of the model consists of an LSTM cell that processes one char at a time and computes probabilities of the possible continuations of the char. \n",
    "            basic_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "            # model.cell.state_size is (128, 128)\n",
    "            self.stacked_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * num_layers)\n",
    "\n",
    "            self.input_data = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"input_data\")\n",
    "            self.targets = tf.placeholder(tf.int32, [batch_size, seq_length], name=\"targets\")\n",
    "            # Initial state of the LSTM memory.\n",
    "            # The memory state of the network is initialized with a vector of zeros and gets updated after reading each char. \n",
    "            self.initial_state = stacked_cell.zero_state(batch_size, tf.float32) #why batch_size\n",
    "\n",
    "            with tf.variable_scope('rnnlm_class1'):\n",
    "                softmax_w = tf.get_variable(\"softmax_w\", [rnn_size, vocab_size]) #128x65\n",
    "                softmax_b = tf.get_variable(\"softmax_b\", [vocab_size]) # 1x65\n",
    "                embedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])  #65x128\n",
    "                inputs = tf.split(tf.nn.embedding_lookup(embedding, self.input_data), seq_length, 1)\n",
    "                inputs = [tf.squeeze(input_, [1]) for input_ in inputs] \n",
    "\n",
    "            # The value of state is updated after processing each batch of chars.\n",
    "            outputs, last_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, self.initial_state, self.stacked_cell, loop_function=None, scope='rnnlm_class1')\n",
    "            output = tf.reshape(tf.concat(outputs,1), [-1, rnn_size])\n",
    "            self.logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "            self.probs = tf.nn.softmax(self.logits)\n",
    "            loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([self.logits],\n",
    "                    [tf.reshape(self.targets, [-1])],\n",
    "                    [tf.ones([batch_size * seq_length])],\n",
    "                    vocab_size)\n",
    "            self.cost = tf.reduce_sum(loss) / batch_size / seq_length\n",
    "            self.final_state = last_state\n",
    "            self.lr = tf.Variable(0.0, trainable=False)\n",
    "            tvars = tf.trainable_variables()\n",
    "            grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars),grad_clip)\n",
    "            optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "            self.train_op = optimizer.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    \n",
    "    def sample(self, sess, chars, vocab, num=200, prime='The ', sampling_type=1):\n",
    "        state = sess.run(self.stacked_cell.zero_state(1, tf.float32))\n",
    "        #print state\n",
    "        for char in prime[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed)\n",
    "\n",
    "        def weighted_pick(weights):\n",
    "            t = np.cumsum(weights)\n",
    "            s = np.sum(weights)\n",
    "            return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "\n",
    "        ret = prime\n",
    "        char = prime[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[char]\n",
    "            feed = {self.input_data: x, self.initial_state:state}\n",
    "            [probs, state] = sess.run([self.probs, self.final_state], feed)\n",
    "            p = probs[0]\n",
    "\n",
    "            if sampling_type == 0:\n",
    "                sample = np.argmax(p)\n",
    "            elif sampling_type == 2:\n",
    "                if char == ' ':\n",
    "                    sample = weighted_pick(p)\n",
    "                else:\n",
    "                    sample = np.argmax(p)\n",
    "            else: # sampling_type == 1 default:\n",
    "                sample = weighted_pick(p)\n",
    "\n",
    "            pred = chars[sample]\n",
    "            ret += pred\n",
    "            char = pred\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"cpu_vs_gpu\"></a>\n",
    "<h2>Train your model using CPU and GPU</h2>\n",
    "We can train our model through feeding batches. You should be able to complete the following cells and submit it for review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_1\"></a>\n",
    "<h2>Question 1: Complete the code to run it on CPU</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/8700 (epoch 0), train_loss = 2.066, time/batch = 0.133\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Mentus--Sund for wheet\n",
      "You a elill tu blow thow thee the fread Thou!\n",
      "Then:\n",
      "Noors\n",
      "\n",
      "fulJ erof and stuem;\n",
      "Conley,\n",
      "We tiriun. and to londst reconnst? the \n",
      "----------------------------------\n",
      "347/8700 (epoch 1), train_loss = 1.862, time/batch = 0.135\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The cale to to mokedy and all moren.\n",
      "\n",
      "BRINCE:\n",
      "Ald my our uther it him groat revousines.\n",
      "\n",
      "KING EDIUS:\n",
      "I will dimane this suldegy?\n",
      "\n",
      "GLUCET:\n",
      "In, porke lit th\n",
      "----------------------------------\n",
      "521/8700 (epoch 2), train_loss = 1.761, time/batch = 0.123\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The with with to to me?\n",
      "A blo?\n",
      "\n",
      "Sersated so have Bundot,.\n",
      "Deke them reas he ilforty?\n",
      "No..\n",
      "\n",
      "CERIXLEO:\n",
      "Not greateaut thus; much urpy to seefins\n",
      "Anclead, Cod\n",
      "----------------------------------\n",
      "695/8700 (epoch 3), train_loss = 1.700, time/batch = 0.160\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The the noble be iton mour like,\n",
      "That makes at a sool's'd; my lrother up the basted, in the ount it ant, and is for apon, you min!\n",
      "Bircal ay reprecelies, \n",
      "----------------------------------\n",
      "869/8700 (epoch 4), train_loss = 1.660, time/batch = 0.139\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The fight the barthe with the kin; these and hersouster, if the sungel, as I Katurty,\n",
      "Bo hon,\n",
      "Caupor For quan wear of Bupareries Ale:\n",
      "Then shall not; for \n",
      "----------------------------------\n",
      "1043/8700 (epoch 5), train_loss = 1.632, time/batch = 0.156\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The toor spuch w-apon.\n",
      "She\n",
      "well my lord; fantony word make not: it shall not daughs vieland, arrow thy disceal. Lodker Romer:\n",
      "Neast, I must pensens of clo\n",
      "----------------------------------\n",
      "1217/8700 (epoch 6), train_loss = 1.611, time/batch = 0.138\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The tendious this grapifs\n",
      "Beforeth that taster's sonfurtest,\n",
      "Which up this, unours, knier'd, of son and, my loved, many am they, it enter\n",
      "ane is now, wher\n",
      "----------------------------------\n",
      "1391/8700 (epoch 7), train_loss = 1.593, time/batch = 0.137\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The mock, the every'p, it would I have holy like me honour: on my vaultings of yours up.\n",
      "\n",
      "RIENO:\n",
      "Blood bow drown, sick, my lord; a duke as on\n",
      "How?\n",
      "\n",
      "COMINI\n",
      "----------------------------------\n",
      "1565/8700 (epoch 8), train_loss = 1.579, time/batch = 0.164\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The grace the streast mast, Servingo of brother's live.\n",
      "\n",
      "HOMI:\n",
      "I think of the labburd.\n",
      "\n",
      "BENVOLIO:\n",
      "Uving upon eet the worshave:\n",
      "Is do the own comply honed,\n",
      "----------------------------------\n",
      "1739/8700 (epoch 9), train_loss = 1.566, time/batch = 0.132\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The neetle self Musticaty.\n",
      "What, soat yept the away?\n",
      "This suld earnain.\n",
      "\n",
      "ROFITAR:\n",
      "Be the vio lost counsitiousperce him me word, good leave you.\n",
      "\n",
      "LUCCITIA:\n",
      "----------------------------------\n",
      "1913/8700 (epoch 10), train_loss = 1.555, time/batch = 0.134\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The coubted as-Horse.\n",
      "\n",
      "KING RICHARD III:\n",
      "For unscumiry in him to stay you; I shore the humper:\n",
      "That; what is an gift on how he caugnt's gentleminither the\n",
      "----------------------------------\n",
      "2087/8700 (epoch 11), train_loss = 1.547, time/batch = 0.160\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The good master his naminess to find lowind 'mughson conset it remondon, but known flongied;\n",
      "Agaused me post Jock,\n",
      "In is not.\n",
      "\n",
      "GREMIO:\n",
      "\n",
      "JULIET:\n",
      "Go alself \n",
      "----------------------------------\n",
      "2261/8700 (epoch 12), train_loss = 1.539, time/batch = 0.152\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The land no mark'st you food life;\n",
      "I had's lights,\n",
      "My ladden sen with one ressea foulther upon sheakly parturn?\n",
      "\n",
      "CLARENCE:\n",
      "Did to the honsent\n",
      "His bust!\n",
      "Ty\n",
      "----------------------------------\n",
      "2435/8700 (epoch 13), train_loss = 1.533, time/batch = 0.122\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The house, being she you, let yield stual be senary to ofter,\n",
      "Wem make affected upon a\n",
      "doors.\n",
      "Whyes, you may about old spirence with this away,\n",
      "Untis jusc\n",
      "----------------------------------\n",
      "2609/8700 (epoch 14), train_loss = 1.527, time/batch = 0.137\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The praison the peace,\n",
      "Musting,---\n",
      "Would, when yet as my purpiete\n",
      "The four here is trick more my could I dew away you are of a feals hand.\n",
      "\n",
      "CORIOLAND:\n",
      "By \n",
      "----------------------------------\n",
      "2783/8700 (epoch 15), train_loss = 1.522, time/batch = 0.159\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The little fainted frolly.\n",
      "\n",
      "SAMPSOR:\n",
      "Is he to thron to farth, we grant's what desued, it erieft.\n",
      "Speak to this foulless which is sughty, by our caficors. \n",
      "----------------------------------\n",
      "2957/8700 (epoch 16), train_loss = 1.518, time/batch = 0.131\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The guilt.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "And I canishvy Was along herefore, and shame gage,\n",
      "To see thy long; whose or our fanten'd man he,\n",
      "Of you so.\n",
      "\n",
      "PROLIO:\n",
      "Where kn\n",
      "----------------------------------\n",
      "3131/8700 (epoch 17), train_loss = 1.513, time/batch = 0.140\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The false but prich achound foul aight.\n",
      "\n",
      "GONZALO:\n",
      "Who more's bow is son,\n",
      "Wherelanished he wilts you. Hence light:\n",
      "I partly, as you inself.\n",
      "\n",
      "ISABELLA:\n",
      "O, s\n",
      "----------------------------------\n",
      "3305/8700 (epoch 18), train_loss = 1.509, time/batch = 0.132\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The kindroum should be much fears;\n",
      "For then with make my happiiant,\n",
      "My natuares to hate,\n",
      "Unand with him but we than worshined or shall I shall in call you\n",
      "----------------------------------\n",
      "3479/8700 (epoch 19), train_loss = 1.505, time/batch = 0.146\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The shall sovere me wilt which after shall the hermina I will place.\n",
      "\n",
      "CLARENCE:\n",
      "How to her ever'd, that, no cooller, hence thou dight\n",
      "To rather gave forge\n",
      "----------------------------------\n",
      "3653/8700 (epoch 20), train_loss = 1.501, time/batch = 0.160\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The bounting; content of my carence arm, the our will this will bids not send, I spept winl's nothing!\n",
      "\n",
      "YORK:\n",
      "Anaves it.\n",
      "\n",
      "KING RICHARD III:\n",
      "Sound that tho\n",
      "----------------------------------\n",
      "3827/8700 (epoch 21), train_loss = 1.498, time/batch = 0.129\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The coarch and heard twentory you be, sayation I do come thou'rt he should plain me, but to thus from their werr:\n",
      "Then I will not thouss be it to my serve\n",
      "----------------------------------\n",
      "4001/8700 (epoch 22), train_loss = 1.494, time/batch = 0.142\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The prepil's bleed of thet were the airy whither;\n",
      "He stiol.\n",
      "\n",
      "RETESBBO:\n",
      "My servant.\n",
      "\n",
      "HENRY BOTNANUS:\n",
      "I do how help a direlly anoth?\n",
      "God nament appose: sir,\n",
      "----------------------------------\n",
      "4175/8700 (epoch 23), train_loss = 1.491, time/batch = 0.150\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The will but that thoughtsolorn:\n",
      "That turth blese profisted?\n",
      "\n",
      "WARW:\n",
      "That's a low youth husbald a Clarence the eash,\n",
      "The king:\n",
      "He wild'd your amish vire, t\n",
      "----------------------------------\n",
      "4349/8700 (epoch 24), train_loss = 1.489, time/batch = 0.131\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The landurt not as grank, as Canning betterness him thy laying a chan.\n",
      "The greaters\n",
      "The thou teston-for the king he fier:\n",
      "Have you that thou cut for my do\n",
      "----------------------------------\n",
      "4523/8700 (epoch 25), train_loss = 1.486, time/batch = 0.168\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The razies, be Doing and king by you!\n",
      "\n",
      "Servancasmaticked with my deight,\n",
      "With the violent will are perpemies,\n",
      "In with thy stoles ask in it no mark.\n",
      "Hath b\n",
      "----------------------------------\n",
      "4697/8700 (epoch 26), train_loss = 1.484, time/batch = 0.140\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The vire! Was\n",
      "Alther witholy buish\n",
      "Sainted, cannot with kind.\n",
      "\n",
      "LADY GREY:\n",
      "Ay, I earble Edward, as you have did neckmys with is o' Sisunce beds the\n",
      "blessel\n",
      "----------------------------------\n",
      "4871/8700 (epoch 27), train_loss = 1.481, time/batch = 0.167\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The olker, hear the rudh sweet; by a rot this ary presenher's sooned,\n",
      "My sword! O, Bolin nimer,\n",
      "Woe. srine to Comer:\n",
      "Then, such her.\n",
      "\n",
      "SOMERLE:\n",
      "Griel abook\n",
      "----------------------------------\n",
      "5045/8700 (epoch 28), train_loss = 1.479, time/batch = 0.139\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The say.\n",
      "\n",
      "COMINIUG:\n",
      "I thou, lany fault thou thinks of hence,' repent the bate.\n",
      "How most thou!\n",
      "Amack maditable live.\n",
      "\n",
      "Second Carrausious;\n",
      "For the sign them\n",
      "----------------------------------\n",
      "5219/8700 (epoch 29), train_loss = 1.477, time/batch = 0.130\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The hour bolly, gave, against in throwd.\n",
      "Here, your good,\n",
      "When thou the kingfulles his before by thee to an evil out all I greator:\n",
      "And that I beseech air\n",
      "----------------------------------\n",
      "5393/8700 (epoch 30), train_loss = 1.475, time/batch = 0.146\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The is. Come! furch'd a shall,\n",
      "And sin you and reventance know?\n",
      "Your dequire bold and volchame:\n",
      "Good his that you now whose with, I so:\n",
      "Plasp recounse\n",
      "I d\n",
      "----------------------------------\n",
      "5567/8700 (epoch 31), train_loss = 1.474, time/batch = 0.139\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The maid thee, speeties:\n",
      "O heaven lament,\n",
      "To eath,\n",
      "As like to justic\n",
      "Known there's ground the tubre, while is in than this?\n",
      "How was the faults and from it\n",
      "----------------------------------\n",
      "5741/8700 (epoch 32), train_loss = 1.472, time/batch = 0.144\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The majesty.\n",
      "\n",
      "ARIEL:\n",
      "Will past away ster, to have seek fro't! They she last of, terry, neither than is the duke\n",
      "I would I was arm,\n",
      "Aim his hears poiting s\n",
      "----------------------------------\n",
      "5915/8700 (epoch 33), train_loss = 1.471, time/batch = 0.158\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The would a propork, keepler:\n",
      "Why, my man bid me to the This now, the joy.\n",
      "\n",
      "GREMIO:\n",
      "How counter'd\n",
      "From ores.\n",
      "\n",
      "LADY ANPO:\n",
      "It\n",
      "he he'ming daughters at overy\n",
      "\n",
      "----------------------------------\n",
      "6089/8700 (epoch 34), train_loss = 1.469, time/batch = 0.134\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The would wiped; as your lord?\n",
      "\n",
      "TYTRA:\n",
      "How had lost fignacioly had long one! 'tis he foud somethin he says are fire of my name\n",
      "His name's than though pres\n",
      "----------------------------------\n",
      "6263/8700 (epoch 35), train_loss = 1.468, time/batch = 0.180\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The would forced.\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "You be not we sled\n",
      "therefore, liesin, I pray the call for the name to chidia to sovered upon my monuch,\n",
      "Whilgirs\n",
      "A t\n",
      "----------------------------------\n",
      "6437/8700 (epoch 36), train_loss = 1.466, time/batch = 0.185\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The news?\n",
      "\n",
      "LUCENTIO:\n",
      "A might.\n",
      "By troth,\n",
      "Bid iBoanty in you depoky.\n",
      "\n",
      "JULIET:\n",
      "Or thoughts tiesimy to time it myself,\n",
      "Wild a shade are you not before by your\n",
      "----------------------------------\n",
      "6611/8700 (epoch 37), train_loss = 1.465, time/batch = 0.165\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The high, not let complay'd so, since with the strengthe! Richard's all tne my mood:\n",
      "A place dishem, as Henry's beary, not disposiling to a virward aod\n",
      "or\n",
      "----------------------------------\n",
      "6785/8700 (epoch 38), train_loss = 1.463, time/batch = 0.138\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Laideness,\n",
      "More masters:\n",
      "Lead to mild goody forth 'twere keakmn war.\n",
      "Hrow'st night\n",
      "In love,\n",
      "Where will doth Rome:\n",
      "Ay, good Caucience to Dure to them; \n",
      "----------------------------------\n",
      "6959/8700 (epoch 39), train_loss = 1.462, time/batch = 0.148\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The country.\n",
      "Lew yourself,\n",
      "More bedied.\n",
      "\n",
      "Peds\n",
      "Sear, were ere their treess, infore from her or old Could plain\n",
      "foocions\n",
      "In swear; to be his banks being you\n",
      "----------------------------------\n",
      "7133/8700 (epoch 40), train_loss = 1.461, time/batch = 0.159\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The word as 'tune of my meeck, or which myself's yourself joy stafter; to revence blood means acts, sir.\n",
      "\n",
      "RICHARD:\n",
      "So plain'd the lives, let they desperis\n",
      "----------------------------------\n",
      "7307/8700 (epoch 41), train_loss = 1.460, time/batch = 0.161\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The left, gentle been\n",
      "inly tongue who,\n",
      "Urquest.\n",
      "\n",
      "MARIANA:\n",
      "Now, wedk is her! what call'd but to rainter and of Sumen to tell; say, will, if you datch.\n",
      "\n",
      "BIO\n",
      "----------------------------------\n",
      "7481/8700 (epoch 42), train_loss = 1.459, time/batch = 0.155\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The attoth else and to be in meet, will one an more something told, Events use the dukeman:\n",
      "Ay, wherear\n",
      "so objoke every lives\n",
      "Bed, it bemight\n",
      "Upon thou kn\n",
      "----------------------------------\n",
      "7655/8700 (epoch 43), train_loss = 1.457, time/batch = 0.167\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The cousin him from his him: contrue before mine,\n",
      "She tot-uate coverte;\n",
      "You heavy base and thy fair usjury; and meth took not when who, again:\n",
      "Why lancher\n",
      "----------------------------------\n",
      "7829/8700 (epoch 44), train_loss = 1.456, time/batch = 0.143\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The dust body where answerdise of Engnator,\n",
      "Sweet yet, and 'twas have reash'd heav'\n",
      "A kind of just as I will you swour eal, and sovereish arm helm interfu\n",
      "----------------------------------\n",
      "8003/8700 (epoch 45), train_loss = 1.455, time/batch = 0.153\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The kings will half soul and elrougession with way\n",
      "Bear thee have by not though we dare\n",
      "With your pringolty uncle-borting tornent.\n",
      "That wouns,\n",
      "But to shal\n",
      "----------------------------------\n",
      "8177/8700 (epoch 46), train_loss = 1.455, time/batch = 0.185\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The Servingman:\n",
      "How hear,\n",
      "As shed wilt deat;\n",
      "Thirt those death; and say:\n",
      "Yea, given to desire.\n",
      "\n",
      "ISABELLA:\n",
      "And deparned the Volsces of one meronamnet.\n",
      "\n",
      "MAR\n",
      "----------------------------------\n",
      "8351/8700 (epoch 47), train_loss = 1.454, time/batch = 0.159\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The carr,\n",
      "Are now the morse thou cancon.\n",
      "\n",
      "Third He will, never words with was this gentle burna fear King dares thee, name loving.\n",
      "\n",
      "LADY BALO:\n",
      "Gid!\n",
      "\n",
      "JULIE\n",
      "----------------------------------\n",
      "8525/8700 (epoch 48), train_loss = 1.453, time/batch = 0.173\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The news should back with the thing mander'd\n",
      "the feeing. Dvermed your beloice,\n",
      "That within must will sourse nevertess!'\n",
      "\n",
      "VI\n",
      "TIANAEL:\n",
      "That be do is\n",
      "I sume \n",
      "----------------------------------\n",
      "8699/8700 (epoch 49), train_loss = 1.452, time/batch = 0.175\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The heir, sword, brother by the kins-what by each'd itabed companion\n",
      "Withou wilt have harvation; what, priciou up.\n",
      "For a criest and more fraling for to o'\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "avg_batch_running_duration_CPU=[]\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"rnn_CPU\"):\n",
    "    model = LSTMModel(device='/cpu:0')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 20 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        ## write your code below to reset the batch pointer in data_loader. you can use reset_batch_pointer()\n",
    "        ##\n",
    "        data_loader.reset_batch_pointer()\n",
    "        \n",
    "        state = sess.run(model.initial_state) # model initialization\n",
    "        batch_running_duration_CPU = []\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            ## write your code to define your x and y. You should use next_batch() from data_loader\n",
    "            ## e.g. x,y =\n",
    "            ##\n",
    "            x, y = data_loader.next_batch()\n",
    "            \n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state: state}\n",
    "            \n",
    "            ## write your code to train the model\n",
    "            ## e.g.: train_loss, state, _ = \n",
    "            ##\n",
    "            sess.run(model.train_op, feed_dict = feed)\n",
    "            train_loss = sess.run(model.cost, feed_dict = feed)\n",
    "            final_state, _ = sess.run(model.final_state, feed_dict = feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            \n",
    "            ## write your code to store the duration of running each batch in a list (end - start)\n",
    "            ##\n",
    "            batch_running_duration_CPU.append(end - start)\n",
    "            \n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        avg_batch_running_duration_CPU.append(sum(batch_running_duration_CPU) / float(len(batch_running_duration_CPU)))\n",
    "        \n",
    "        # Please uncomment the following block of the code so the grader can see the sample of prediction\n",
    "        with tf.variable_scope(\"rnn_CPU\", reuse=True):\n",
    "            sample_model = LSTMModel(sample=True)\n",
    "            print ('----------------------------------')\n",
    "            print ('SAMPLE GENERATED TEXT:')\n",
    "            print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=150, prime='The ', sampling_type=1))\n",
    "            print ('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_2\"></a>\n",
    "<h2>Question 2: Complete the code to run it on GPU</h2>\n",
    "Now, create the same network with GPU, and calculate the time/batch for running each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173/8700 (epoch 0), train_loss = 2.060, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The bus liigest:\n",
      "His whis menelirter,\n",
      "as erone stou more my iill tur,--'id The\n",
      "I ma; Shere our hars, poo hear has:\n",
      "Stow woll bping; Chy soud or You swall \n",
      "----------------------------------\n",
      "347/8700 (epoch 1), train_loss = 1.857, time/batch = 0.056\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The saithes, the viremis, gighy-lakem,\n",
      "whith like hes onterupeitherd sounten? to mun he pot beep!\n",
      "\n",
      "GXUSOENBANLI:\n",
      "But Magrow: for the intinf.\n",
      "Hes shill the\n",
      "----------------------------------\n",
      "521/8700 (epoch 2), train_loss = 1.755, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The were your his incing comen.\n",
      "\n",
      "NWARWARD: I hop our was ye than and Lort at the bonnduient,\n",
      "Whurd, juds plean tean me sosen perpuce that here leave a wel\n",
      "----------------------------------\n",
      "695/8700 (epoch 3), train_loss = 1.695, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The 'Twast be welnow.\n",
      "Sfuminiends' tit.\n",
      "\n",
      "Counter Pallought as; speak and treparud their, son thubly, to speak I may,, 'Dick stemporeast our bearveng celes\n",
      "----------------------------------\n",
      "869/8700 (epoch 4), train_loss = 1.657, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The every for thee.\n",
      "\n",
      "PRI:\n",
      "Gostory fouth end thee sIONIO:\n",
      "Lets a jays.\n",
      "Lefe that tabser, pary'le is the gairs\n",
      "TIO worthy of again.\n",
      "\n",
      "POUVELBOUTES:\n",
      "You hourt\n",
      "----------------------------------\n",
      "1043/8700 (epoch 5), train_loss = 1.628, time/batch = 0.056\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The acthour this is.\n",
      "Cactwer las her mare with's sto, let you hear:\n",
      "Ap in that never are my forboon unsain\n",
      "Earch for will thou take him but smalf of hisNo\n",
      "----------------------------------\n",
      "1217/8700 (epoch 6), train_loss = 1.605, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The gratts, each upon gion law's\n",
      "Away; hear it; but is grace\n",
      "Before abusine wrathsom fars tongerispos, bory ad ont sir:\n",
      "Yor giof lonlo she shaply is hast \n",
      "----------------------------------\n",
      "1391/8700 (epoch 7), train_loss = 1.586, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The placius.\n",
      "\n",
      "BIXKISY:\n",
      "Me\n",
      "tome live\n",
      "\n",
      "shepper;\n",
      "The untolious prast the fieves, nate so provelly, to just and swrowted to your fathich Is do you! Shall kiss\n",
      "----------------------------------\n",
      "1565/8700 (epoch 8), train_loss = 1.571, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wand my walsiss from us is did thy narm,\n",
      "He should had clove grace good apprach'd strijes\n",
      "No it\n",
      "ere it, and needers\n",
      "And we? sleek banishm be permencif\n",
      "----------------------------------\n",
      "1739/8700 (epoch 9), train_loss = 1.558, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The stopings' twase,\n",
      "And is sterely, to his grafed.\n",
      "\n",
      "LUCIO:\n",
      "No, are with anown?\n",
      "\n",
      "QUCENLO:\n",
      "That arry. She's other, what.\n",
      "\n",
      "First Miny your most.\n",
      "\n",
      "YORK:\n",
      "Go w\n",
      "----------------------------------\n",
      "1913/8700 (epoch 10), train_loss = 1.547, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The glast,\n",
      "Had all in my inforest chamed, he, for these wander is suption of your daughter, are good such a thousing name, and him answick'd and while is \n",
      "----------------------------------\n",
      "2087/8700 (epoch 11), train_loss = 1.537, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The ply in trown him.\n",
      "\n",
      "KIND LOWINGO:\n",
      "Ay, he so was shall viario?\n",
      "\n",
      "LORGELIUT:\n",
      "I that 'tis wanter takes the fich with the night,\n",
      "How thou this earth\n",
      "To abue\n",
      "----------------------------------\n",
      "2261/8700 (epoch 12), train_loss = 1.529, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The king.\n",
      "\n",
      "VOLUMNIAN:\n",
      "O, the thight, but sucled to whild prince,\n",
      "For sucting;\n",
      "Gring that he to be sir! I had let's a change walker this any, whom but him-\n",
      "----------------------------------\n",
      "2435/8700 (epoch 13), train_loss = 1.521, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wlay you plack.\n",
      "Mowend you can the noble but tome should suy fieremity,\n",
      "That dream?\n",
      "\n",
      "ANTIO:\n",
      "I will as I be thousing,\n",
      "A morts,\n",
      "And, yet Condeman's dead\n",
      "----------------------------------\n",
      "2609/8700 (epoch 14), train_loss = 1.515, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The might known Come\n",
      "closs, are's even hear. Thy emast to throw on their soul arlife, what shall hear the fled;\n",
      "As't in your thought out your grace,\n",
      "Is sh\n",
      "----------------------------------\n",
      "2783/8700 (epoch 15), train_loss = 1.509, time/batch = 0.056\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The lass,\n",
      "And the into this faith the suckle deperf;\n",
      "O lischest child, and his grier, my truth me, uncle adors, unquing hurpose the trown will report, by \n",
      "----------------------------------\n",
      "2957/8700 (epoch 16), train_loss = 1.505, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The pience, were liege, what God but royal the pricaying of perish'd to him? Call me come in in broeds.\n",
      "\n",
      "AEdizen:\n",
      "Marcking freth down the business of all'\n",
      "----------------------------------\n",
      "3131/8700 (epoch 17), train_loss = 1.501, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The while?\n",
      "\n",
      "BAPTISTA:\n",
      "'Tamp house thee.\n",
      "\n",
      "GONZALOS:\n",
      "O lustor:\n",
      "I shall be none?\n",
      "\n",
      "TRANIO:\n",
      "Even we tack me a trung?\n",
      "\n",
      "Hoo cause thee, nor tear.\n",
      "\n",
      "AERLAND:\n",
      "You s\n",
      "----------------------------------\n",
      "3305/8700 (epoch 18), train_loss = 1.497, time/batch = 0.056\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The duke we alts as tuo.\n",
      "\n",
      "PRINCE Edward him!\n",
      "\n",
      "VINCENTIO:\n",
      "Will this ucellain.\n",
      "Why, peor thy work?\n",
      "\n",
      "CLORET:\n",
      "\n",
      "KING RICHARD III:\n",
      "I tell so rother,\n",
      "As any\n",
      "In't\n",
      "----------------------------------\n",
      "3479/8700 (epoch 19), train_loss = 1.493, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The streign, let her wherefore the ends had I am say my lord;\n",
      "For his brother.\n",
      "Beseer thy lady, in hine, you well: catlady the Luppies word,\n",
      "They\n",
      "Who, tea\n",
      "----------------------------------\n",
      "3653/8700 (epoch 20), train_loss = 1.490, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The withle weiry, the majain ofter\n",
      "And ime and lope, I therefore 'twant Bickings; but we shall go tray you not\n",
      "Will we eyed are now eftel it removeet the \n",
      "----------------------------------\n",
      "3827/8700 (epoch 21), train_loss = 1.487, time/batch = 0.056\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The garing us a king and charqures urtime fees answer smeef!\n",
      "Good mishonour?\n",
      "Sail offectied:\n",
      "Which to my heart;\n",
      "Yet no name, leave,\n",
      "And hear there! Ay, fa\n",
      "----------------------------------\n",
      "4001/8700 (epoch 22), train_loss = 1.484, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The salful apon their love,\n",
      "Who\n",
      "We'll\n",
      "\n",
      "Severtin'd the quects\n",
      "Thou littens.\n",
      "\n",
      "BIONDELLO:\n",
      "Then as longy with such had beforeme me\n",
      "Ten your enemys; 'tis no sc\n",
      "----------------------------------\n",
      "4175/8700 (epoch 23), train_loss = 1.481, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The judvent:\n",
      "I would fear the dight\n",
      "me youth may'l.\n",
      "For, no devied to bessensing with me.\n",
      "\n",
      "CORIOLANUS:\n",
      "How in his convey to be so east?\n",
      "\n",
      "LOON:\n",
      "True leosed\n",
      "----------------------------------\n",
      "4349/8700 (epoch 24), train_loss = 1.479, time/batch = 0.056\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The quathers and confess? I will ears' accascationy, and be vengean to kiss the bodal sevent\n",
      "Should toothy both state;\n",
      "For Bolimpared my braw; ball, made \n",
      "----------------------------------\n",
      "4523/8700 (epoch 25), train_loss = 1.477, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The firmbling enterplord;\n",
      "For this\n",
      "deed tongue lies, Sight bory love it it?\n",
      "Cachard laid I henry acreckep'd 'ncouch good citizens,\n",
      "Than he that worth ear \n",
      "----------------------------------\n",
      "4697/8700 (epoch 26), train_loss = 1.474, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The mild but servised:\n",
      "To cannot see with thie;\n",
      "My; and now: yet not thou,\n",
      "Should be resamon man!\n",
      "He were thy angriries of ybully by a curnible charge,\n",
      "We\n",
      "----------------------------------\n",
      "4871/8700 (epoch 27), train_loss = 1.472, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The larch: seek my lord.\n",
      "My led-more capulety,: but lady's some, and you to the desced's merry it true, broke on fie;\n",
      "Lege\n",
      "To she farewell, kepedine again\n",
      "----------------------------------\n",
      "5045/8700 (epoch 28), train_loss = 1.470, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The fly werver do not precour inford.\n",
      "\n",
      "AUTOLYCUS:\n",
      "I wow be groag!\n",
      "Mine issues no place!\n",
      "\n",
      "LEONTES:\n",
      "Your land of them,\n",
      "Thou is as I woe, I knows Enword, I b\n",
      "----------------------------------\n",
      "5219/8700 (epoch 29), train_loss = 1.469, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The beet dolm:\n",
      "If these swold are at on you forerour.\n",
      "She say?\n",
      "\n",
      "DUKE OF YORK:\n",
      "Why, thy fair some confiter'st\n",
      "And this heavensage, shall well, Cloilt.\n",
      "What\n",
      "----------------------------------\n",
      "5393/8700 (epoch 30), train_loss = 1.467, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The presure\n",
      "As one to-morrow,\n",
      "And should say, sir, but\n",
      "this shall thy true;\n",
      "Shrown of his nortching; I cannot usin't fee-inet his heart\n",
      "Tall'd these; bowl\n",
      "----------------------------------\n",
      "5567/8700 (epoch 31), train_loss = 1.465, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The I bepittagues, imbandant! Ky recompt'd eld wisheff how to go.\n",
      "\n",
      "Nurse:\n",
      "Ratch sweet refeitle to but as one-deorth we can cleth no royal to put him hung \n",
      "----------------------------------\n",
      "5741/8700 (epoch 32), train_loss = 1.464, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The dead to Gentleman; 'tis in me, I was a from go the warb the marthed on more of the strick,\n",
      "If talk the servance Fresp and his moon.\n",
      "\n",
      "PETRUCHIO:\n",
      "Ay, wh\n",
      "----------------------------------\n",
      "5915/8700 (epoch 33), train_loss = 1.462, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The grounds I him a last woa by your comply happy thou now,\n",
      "Let die.\n",
      "What care not?\n",
      "\n",
      "HORTENSIO:\n",
      "Spoke have bear, and be nerent,\n",
      "Your tears is of a deceive\n",
      "----------------------------------\n",
      "6089/8700 (epoch 34), train_loss = 1.461, time/batch = 0.054\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The parting: for you; now devil devair klair\n",
      "Honound preservied chieffiel'd,\n",
      "And will me wants which,\n",
      "And if thou is't?\n",
      "\n",
      "ROMEO:\n",
      "I mark in well ass too ins\n",
      "----------------------------------\n",
      "6263/8700 (epoch 35), train_loss = 1.459, time/batch = 0.056\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wife seases creedure neur friend here\n",
      "Ned 'tongue it added!\n",
      "\n",
      "HORTENSIO:\n",
      "Your dazerry,\n",
      "That KING RICHARD III:\n",
      "Sungman:\n",
      "Master. Plucks a stone pleasince\n",
      "----------------------------------\n",
      "6437/8700 (epoch 36), train_loss = 1.458, time/batch = 0.056\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The good growns in contomeet?\n",
      "Oor, you verd ear, lovely are you subpute:\n",
      "Antage strange,\n",
      "That? it shephervend to be our reliverry hanns;\n",
      "Seell tell with-w\n",
      "----------------------------------\n",
      "6611/8700 (epoch 37), train_loss = 1.457, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The ney.\n",
      "\n",
      "SARELDUTIO:\n",
      "Gool,\n",
      "Tell the garew conslingly of sheck, not it.\n",
      "\n",
      "KING HENRY BOLINGBROKE:\n",
      "Thou watch no Cluse of:\n",
      "Ay, to the good pauldua, you stop\n",
      "----------------------------------\n",
      "6785/8700 (epoch 38), train_loss = 1.455, time/batch = 0.058\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The greeful gaid of youth to know\n",
      "I would be no gar to him, be you, forbear.\n",
      "\n",
      "First make a scond part\n",
      "Did I am came can one the fapertural, for his fors, \n",
      "----------------------------------\n",
      "6959/8700 (epoch 39), train_loss = 1.454, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The veryshing it may of what, tirle,\n",
      "Tepuls shake me soin been.\n",
      "\n",
      "God:\n",
      "I say:\n",
      "Remeoso Ninsume them with point we say be and tight srucks therein yonder of \n",
      "----------------------------------\n",
      "7133/8700 (epoch 40), train_loss = 1.453, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The heaven lets by done to his brother,\n",
      "That, I cards me all streces be preposed as it shall be boat you voser is out of signior villament;\n",
      "And unto honou\n",
      "----------------------------------\n",
      "7307/8700 (epoch 41), train_loss = 1.452, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The foil shall: makes me, I fediur!\n",
      "\n",
      "BUCKINGHAM:\n",
      "I ina daughter in the feince is the rood,\n",
      "Would thinking talk ston, the whereof a soul mine oat nor disgu\n",
      "----------------------------------\n",
      "7481/8700 (epoch 42), train_loss = 1.451, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The heaven free.\n",
      "If I to stands\n",
      "That have mine to better, go bring though that just to more to alack; if your countented his holdbitire cold, what were th\n",
      "----------------------------------\n",
      "7655/8700 (epoch 43), train_loss = 1.450, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The dreasure,\n",
      "For Edward.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Speak of that, and enteek, I were shalloyards he cover mercy\n",
      "This monour. If death\n",
      "That never love't us keser; I wo\n",
      "----------------------------------\n",
      "7829/8700 (epoch 44), train_loss = 1.449, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The wills indole first\n",
      "Dear lend at biddaught me to graties;\n",
      "Anded what may not I wave and name haste Biard,\n",
      "And else would heart? thereffatter,\n",
      "And leave\n",
      "----------------------------------\n",
      "8003/8700 (epoch 45), train_loss = 1.448, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The matce's sovereign butter am a very when itself?\n",
      "\n",
      "HORTENSIO:\n",
      "Would not be give me a woes them I say\n",
      "this wrong.\n",
      "\n",
      "MENENIUS:\n",
      "What is impositted awaful so\n",
      "----------------------------------\n",
      "8177/8700 (epoch 46), train_loss = 1.448, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The worse will all order seven-taight, Paulful,\n",
      "Let the followio.\n",
      "\n",
      "JULIET:\n",
      "Can\n",
      "Ha seat to seem up thereto earlen:\n",
      "I beserw'st me their wounds foise!\n",
      "\n",
      "KING\n",
      "----------------------------------\n",
      "8351/8700 (epoch 47), train_loss = 1.447, time/batch = 0.055\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The threement these, answer Proffuder. Blaker mar no, beoper'd till all from appear'd?\n",
      "\n",
      "Servold up you?\n",
      "\n",
      "KING EDWARD IV:\n",
      "How being young steel the wrongs\n",
      "\n",
      "----------------------------------\n",
      "8525/8700 (epoch 48), train_loss = 1.446, time/batch = 0.056\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The heal\n",
      "Pracious Duke owe coususted\n",
      "Will not plaches of that too.\n",
      "\n",
      "OXFORD:\n",
      "I may sound not before my patted.\n",
      "\n",
      "Thirvello climent these;\n",
      "Tare queen young t\n",
      "----------------------------------\n",
      "8699/8700 (epoch 49), train_loss = 1.445, time/batch = 0.053\n",
      "----------------------------------\n",
      "SAMPLE GENERATED TEXT:\n",
      "The brother,\n",
      "For stable 's spribely? stings moon old gets.\n",
      "Hethon, belwitt titly beguing's for all thy nead.\n",
      "This whisht wither? look here? wrough there,\n",
      "\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Create the same network using GPU\n",
    "\n",
    "avg_batch_running_duration_GPU=[]\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope(\"rnn_GPU\"):\n",
    "    model = LSTMModel(device='/gpu:0')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for e in range(num_epochs): # num_epochs is 20 for test, but should be higher\n",
    "        sess.run(tf.assign(model.lr, learning_rate * (decay_rate ** e)))\n",
    "        ## write your code below to reset the batch pointer in data_loader. you can use reset_batch_pointer()\n",
    "        ##\n",
    "        data_loader.reset_batch_pointer()\n",
    "        \n",
    "        state = sess.run(model.initial_state) # model initialization\n",
    "        batch_running_duration_GPU = []\n",
    "        for b in range(data_loader.num_batches): #for each batch\n",
    "            start = time.time()\n",
    "            ## write your code to define your x and y. You should use next_batch() from data_loader\n",
    "            ## e.g. x,y =\n",
    "            ##\n",
    "            x, y = data_loader.next_batch()\n",
    "            \n",
    "            feed = {model.input_data: x, model.targets: y, model.initial_state:state}   \n",
    "            \n",
    "            ## write your code to train the model\n",
    "            ## e.g.: train_loss, state, _ = \n",
    "            ##\n",
    "            sess.run(model.train_op, feed_dict = feed)\n",
    "            train_loss = sess.run(model.cost, feed_dict = feed)\n",
    "            final_state, _ = sess.run(model.final_state, feed_dict = feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            ## write your code to store the duration of running each batch in a list (end - start)\n",
    "            ##\n",
    "            batch_running_duration_GPU.append(end - start)\n",
    "            \n",
    "        print(\"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.num_batches + b, num_epochs * data_loader.num_batches, e, train_loss, end - start))\n",
    "        avg_batch_running_duration_GPU.append(sum(batch_running_duration_GPU) / float(len(batch_running_duration_GPU)))\n",
    "        \n",
    "        # Please uncomment the following block of the code so the grader can see the sample of prediction\n",
    "        with tf.variable_scope(\"rnn_GPU\", reuse=True):\n",
    "            sample_model = LSTMModel(sample=True)\n",
    "            print ('----------------------------------')\n",
    "            print ('SAMPLE GENERATED TEXT:')\n",
    "            print (sample_model.sample(sess, data_loader.chars , data_loader.vocab, num=150, prime='The ', sampling_type=1))\n",
    "            print ('----------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"question_3\"></a>\n",
    "<h2>Question 3: Compare the results</h2>\n",
    "Finally, using a graph, show the speed of training (batch/time) for the model running on GPU and CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Write your code here\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(data_loader.num_batches), batch_running_duration_CPU, label = 'CPU')\n",
    "plt.plot(range(data_loader.num_batches), batch_running_duration_GPU, label = 'GPU')\n",
    "plt.title('CPU vs. GPU Training Speed (sec/batch)')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Time (sec)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<h2>Do you want to use GPU in production?</h2>\n",
    "\n",
    "<p>Running deep learning programs usually needs a high performance platform. PowerAI speeds up deep learning and AI. Built on IBM's Power Systems, PowerAI is a scalable software platform that accelerates deep learning and AI with blazing performance for individual users or enterprises. The <a href=\"https://cocl.us/ML0122EN_IBMCLOUD_PowerAI\">PowerAI platform on IBM Cloud</a> supports popular machine learning libraries and dependencies including TensorFlow, Caffe, PyTorch, and Theano.</p>\n",
    "\n",
    "<h3>Thanks for completing this lesson!</h3>\n",
    "\n",
    "\n",
    "\n",
    "<h4>Author:  <a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>,   <a href=\"https://www.linkedin.com/in/yi-leng-yao-84451275/\">Yi leng Yao</a></h4>\n",
    "<p><a href=\"https://ca.linkedin.com/in/saeedaghabozorgi\">Saeed Aghabozorgi</a>, PhD is a Data Scientist in IBM with a track record of developing enterprise level applications that substantially increases clients ability to turn data into actionable knowledge. He is a researcher in data mining field and expert in developing advanced analytic methods like machine learning and statistical modelling on large datasets.</p>\n",
    "</article>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<hr>\n",
    "\n",
    "<p>Copyright &copy; 2018 <a href=\"https://cocl.us/DX0108EN_CC\">Cognitive Class</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\">MIT License</a>.</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
